\subsubsection{Best practices and design choices}
\label{sub:best_practices_and_design_choices}

This subsection will list and describe important metrics, heuristics and design choices that are either fundamentally important for deep learning or that will be used throughout this work.

\paragraph{Optimization}
Here I talk about Adam and better gradient techniques such as gradient clipping, dropout(?) and residual connections.

\paragraph{Sampling strategies}
Write about beam search, top-k and nucleus sampling.

\paragraph{Regularization and normalization}
Write about dropout and label smoothing as well as batch and layer normalization

\paragraph{Perplexity}
Write about perplexity.

\paragraph{Transfer learning and fine-tuning}
Currently, for many machine learning tasks, more computing power seems to lead predictably to better performance, and is often complementary to algorithmic advances. This is a statement that has never been so true as it is now. Consequently, training a current state-of-the-art model requires signficant hardware and data resources. While most available \gls{nlp} models ten years ago could be developed and trained with commercial laptops or servers, nowadays much specialized hardware in form of GPUs and TPUs is needed (although research for fast CPU algorithms is conducted as well~\footnote{\url{https://hothardware.com/news/researchers-slide-algorithm-cpu-ai-training-outperforms-gpu}}). One way to face this challenge and still achieve high performance results is through the field of sequential \textbf{transfer learning}, where tasks are learned successively. Transfer learning allows models to be trained for similar tasks or domains to increase their generalization capabilities. This can then lead to models achieving better results in areas outside of their initial specification. Sequential transfer learning comprises two phases: During the \textit{pretraining} phase, the model is trained to learn very general representations for a source domain. Afterwards, in the \textit{adaptation} phase, the model is specifically trained to perform well on a (usually more narrow) target domain. This adaptation, or fine-tuning, phase introduces a minimal set of task-specific parameters for subsequent tasks. In contrast to the transfer learning approach, classical models perform isolated learning of a single prediction model for a certain task - this is mainly suitable for well-defined, narrow tasks and requires training of the model from scratch. \\
Pretraining of \gls{lm}s has proven to be an effective method for improving many \gls{nlp} tasks~\footcite{DBLP:journals/corr/abs-1801-06146}. This shift also allows for an efficient and resource-saving adaptation to the target task while at the same time maintaining (or increasing) performance. Another advantage is that the development of \gls{nlp} systems is accelerated by more easily reusable and better-known systems.
