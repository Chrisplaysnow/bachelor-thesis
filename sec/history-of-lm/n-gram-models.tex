\subsubsection{N-Gram models}
\label{sub:n_gram_models}

One solution in dealing with the problem of predicting a word after a sequence of $ (n - 1) $
words in the form of a Markov model, i.e. the probability of each event depends only on the state
attained through the previous event, is called an n-gram model. An “n-gram” hereby denotes a chunk 
of n consecutive words. The core idea is that the probability of a word $ w_i $ occurring in the 
$ i^{th} $ instance after a sequence of $ (i - 1) $ preceding words can be approximated by observing 
only the preceding context of $ (n - 1) $ words.

Following this insight we can compute the probability of all n-grams in a corpus of text by simply counting their occurrences.
Doing so allows us to calculate these conditional probabilities like so:
\begin{align}
    \begin{split}
        P(x^{(t+1)} | x^{(t)}, \dots, x^{(1)}) &= P(x^{(t+1)} | x^{(t)}, \dots, x^{(t - 2 + 2)}) \\ \\
        &= \frac{P(x^{(t+1)}, x^{(t)}, \dots, x^{(t - 2 + 2)})}{P(x^{(t)}, \dots, x^{(t - 2 + 2)})} \\ \\
        &\approx \frac{\text{count}(x^{(t+1)}, x^{(t)}, \dots, x^{(t - n + 2)})}{\text{count}(x^{(t)}, \dots, x^{(t - n + 2)})} \text{(statistical approximation)}
    \end{split}
\end{align}
N-Gram Models where n=1 are called Unigram, n=2 Bigram and n=3 respectively Trigram.

[Talk about the text generation process]

Even though n-gram models have been widely used especially due to their simplicity and scalability 
they do face certain limitations that have led to a decrease in their popularity. One problem that 
can arise when computing n-gram frequencies/probabilites is that n-grams encountered in a test setting 
do not appear in the corpus that the model was trained on. This leads to the probability of that n-gram being 0 
[insert formula]. In order to counter this different smoothing techniques can be applied. 

Sparsity: what if “students opened their w” or “students opened their'' never occurred? solutions -> smoothing, backoff [cite stanford textbook chapter]

Storage: Need to store counts for all n-grams you saw in the corpus. NO SOLUTION.

Lack of understanding: Perhaps the biggest drawback of n-gram models, though, is their very limited context size. In practice no n>5 usually. While produced output is often grammatically correct, there is an evident [lack] of coherence.

“today the price of gold per ton , while production of shoe lasts and shoe industry , the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks , sept 30 end primary 76 cts a share”

Even though n-gram language models are still widely used in speech recognition due to their high efficiency in inference, their limitations caused by poor generalization to unobserved n-grams and inability to capture long range dependencies led to the rise of neural language models [ice paper 8].
