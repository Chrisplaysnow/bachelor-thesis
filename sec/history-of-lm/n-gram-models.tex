\subsubsection{N-Gram models}
\label{sub:n_gram_models}

One solution in dealing with the problem of predicting a word after a sequence of $ (n - 1) $
words in the form of a Markov model, i.e. the probability of each event depends only on the state
attained through the previous event, is called an n-gram model. An “n-gram” hereby denotes a chunk 
of n consecutive words. The core idea is that the probability of a word $ w_i $ occurring in the 
$ i^{th} $ instance after a sequence of $ (i - 1) $ preceding words can be approximated by observing 
only the preceding context of $ (n - 1) $ words.

Following this insight we can compute the probability of all n-grams in a corpus of text by simply counting their occurrences.
Doing so allows us to calculate the conditional probabilities like so:
\begin{align}
    \begin{split}
        P(x^{(t+1)} | x^{(t)}, \dots, x^{(1)}) &= P(x^{(t+1)} | x^{(t)}, \dots, x^{(t - 2 + 2)}) \\ \\
        &= \frac{P(x^{(t+1)}, x^{(t)}, \dots, x^{(t - 2 + 2)})}{P(x^{(t)}, \dots, x^{(t - 2 + 2)})} \\ \\
        &\approx \frac{\text{count}(x^{(t+1)}, x^{(t)}, \dots, x^{(t - n + 2)})}{\text{count}(x^{(t)}, \dots, x^{(t - n + 2)})}
    \end{split}
\end{align}
N-gram models where $ n = 1 $, $ n = 2 $ and $ n = 3 $ are called unigram, bigram and trigram, respectively. The parameter $ n $ typically does not get bigger than $ 5 $. Having the conditional probabilites new text can be generated by conditioning on the provided input. After getting a probability distribution over the vocabulary a sampling strategy (ch. 3.3.3 [HARDCODED]) that returns a word has to be applied.

Even though n-gram models have been widely used especially due to their simplicity and scalability they do face certain limitations that have led to a decrease in their popularity. One problem that can occur is the one of unexpected n-grams: if the $ n $-gram encountered in the test setting did not appear in the corpus that the model was trained on, then the probability for the $ n^{th} $ word conditioned on the $ (n-1) $ words is $ 0 $. If an $ (n-1) $-gram is encountered in the test setting but not in the training data, then the model can not calculate the probability of any word that comes after. These two sparsity limitations can, however, partly be encountered by using smoothing and backoff techniques. The storage problem is evident, as increasing $ n $ or simply enlarging the training corpus increases the model size, which poses a significant problem for larger NLP applications. The implications of this restriction can be withdrawn from the following illustrative generated text snippet:

\begin{quote}
“today the price of gold per ton , while production of shoe lasts and shoe industry , the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks , sept 30 end primary 76 cts a share”
\end{quote}

Because of the inability to expand the context window effectively, we remain tied to a LM that can generate grammatical but also incoherent model that can not relate predictions to history reaching further into the past. N-gram language models are still widely used in speech recognition due to their high efficiency in inference~\footcite{wang2019improving} but their limitations caused by poor generalization to unobserved n-grams and inability to capture long range dependencies led to the rise of neural language models.



