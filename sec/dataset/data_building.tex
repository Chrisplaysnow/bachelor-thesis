\subsubsection{Data Building}
\label{sub:data_building}

After downloading the Wikipedia dumps with a compressed size of 17GB the natural text was extracted via the \textit{wikiextractor}\footnote{\url{https://github.com/attardi/wikiextractor}} library (filtering out texts with a character length less than 1000) and the metadata was extracted and parsed using \textit{SAX Parser}\footnote{\url{https://docs.python.org/3/library/xml.sax.reader.html}} (“Simple API for XML”). In order to finalize the data set creation, synthetic text had to be generated for each article’s first 60 characters twice.
The double generation was performed in order to select the sample that GPT-2 felt more confident about and improve the data set quality.

Given the aforementioned parameter configurations, the first pipeline runs took place in Google Colaboratory\footnote{\url{https://colab.research.google.com/notebooks/intro.ipynb}}, a free jupyter notebook cloud hosting platform which provides users with a GPU, in this case, an Nvidia Tesla K80\footnote{\url{https://www.nvidia.com/en-gb/data-center/tesla-k80/}}. For the generation of synthetic text, the large GPT-2 model (774M parameters) was used. The generation of each text snippet took about 11 seconds. As soon as the pipeline was fully functioning the jupyter notebooks were converted into python modules and the code was transferred to a Google Cloud\footnote{\url{https://cloud.google.com/}} deep learning virtual machine\footnote{\url{https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning?_ga=2.99839453.-2121832178.1549923708}} (VM) with 13GB of memory, a 100GB standard persistent disk, and 2 Nvidia K80 GPUs. Google Cloud was elected for the reason of providing users with a free credit of 300\$, while other cloud computing providers such as AWS\footnote{\url{https://aws.amazon.com/}} or Azure\footnote{\url{https://azure.microsoft.com/en-us/features/azure-portal/}} only provide a free credit of 100\$. Furthermore, the specialized deep learning VM was chosen as it is configured to support common GPU workloads out of the box and removes the task of setting up a high-performance computing environment by having all the needed libraries (e.g. PyTorch, CUDA) and corresponding drivers preinstalled and with a cost of 295.20\$ per month also being less expensive than choosing the same hardware configuration oneself with a cost of about 500\$ per month.

The creation of the data points was parallelized and the total time needed for the dataset creation was approximately 14 days. The data points and log files were stored daily in a Google Storage Bucket\footnote{\url{https://cloud.google.com/storage/docs/json_api/v1/buckets}} and could then be downloaded into a local machine for manual inspection.

The finalized JSON format of a data point is shown in Listing~\ref{lst:data_json}. The key "\textit{label}" indicates whether the text entry was created by a human (0) or by GPT-2 (1).\\

\begin{lstlisting}[language=json,firstnumber=1,label={lst:data_json},caption={Example of a data point}]
{
	"meta": {
		"id": "565318",
		"input": "Lost in Space is a 1998 American science-fiction adventure",
		"pageviews": 57062,
		"touched": "2020-02-26T06:53:05Z",
	},
	"label": 0,
	"title": "Lost in Space (film)",
	"text": "Lost in Space is a 1998 American science-fiction adventure film directed by Stephen Hopkins, and starring William Hurt, Matt LeBlanc, and Gary Oldman. The plot is adapted from the 1965-1968 CBS television series \"of the same name\". Seve"
}
\end{lstlisting}