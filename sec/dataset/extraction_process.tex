\subsubsection{Extraction Process}
\label{sub:extraction_process}

The two usual ways of scraping Wikipedia articles and other data like article metadata or media files from the Wikimedia Foundation\footnote{\url{https://wikimediafoundation.org/}} are through the APIs or the database dumps provided by the MediaWiki platform\footnote{\url{https://www.mediawiki.org/wiki/MediaWiki}}.
Both of these channels were used for different purposes:
The \textbf{database dumps} in the form of 17GB of compressed XML files were firstly downloaded and then titles, clear text and article ids were extracted using a python library called WikiExtractor\footnote{\url{https://github.com/attardi/wikiextractor}}. The output was stored in “.jsonl” (JSON lines) files, where each line denotes a complete JSON object. As the total size of the required metadata is a lot smaller, the \textbf{MediaWiki API} was used to retrieve information such as page views, the latest edit timestamp and categories linked to each article via the extracted article ids. Furthermore the API was used to access the latest news listed in WikiNews to generate exemplary news messages.

The extension of text samples by metadata was done for the purpose of examining three different hypotheses. \textbf{Firstly}, the assumption is being made that the detection rate varies (significantly) across different categories. \textbf{Secondly}, it is expected that the higher the edits and/or views on a page are, the lower the detection rate will be as the human text will be more sophisticated and better worded. \textbf{Thirdly}, the more recent the last edit timestamp of an article is, the lower the detection rate will be as newer information will be less likely to be present in the training data used for \gls{gpt2}. Furthermore, only articles with a minimum length of 1000 characters were taken into consideration as a measure to prevent a decrease in dataset quality (e.g. to filter out entries that only have a redirect notice to another article). For each article entry the last edit timestamp, the aggregated amount of page views over the last 60 days and the namespace was retrieved.

% \begin{itemize}
%     \item{\textbf{Database Dumps:}} Titles, clear text and article ids were extracted using a python library called WikiExtractor\footnote{\url{https://github.com/attardi/wikiextractor}}. MediaWiki provides Wikipedia database dumps in form of compressed XML files can be downloaded and then parsed by WikiExtractor. The output is stored in “.jsonl” (JSON lines) files, where each line denotes a complete JSON object.
%     \item{\textbf{Api:}} The MediaWiki API was used to retrieve metadata such as page views, the latest edit timestamp and categories linked to each article via the extracted page ids, but also to access the latest news listed in WikiNews to generate exemplary news messages.
% \end{itemize}