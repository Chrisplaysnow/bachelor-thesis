\subsubsection{Data Extraction}
\label{sub:data_extraction}

The two most used ways of scraping Wikipedia articles and other data from the Wikimedia Foundation 
[cite website] are through the APIs [cite] or the database dumps provided by the MediaWiki platform [ref site]. 
Both of these channels were used for different purposes:

\begin{itemize}
    \item{\textbf{Database Dumps:}} Titles, clear text and article id were extracted using a python library called WikiExtractor [ref extractor]. 
    This tool parses the xml-formatted dumps and extracts the aforementioned fields. The output is stored in “.jsonl” (json lines) 
    files [ref json lines], as this is a file format commonly used for nlp data [reference].
    \item{\textbf{Api:}} The MediaWiki Api was used to retrieve metadata such as pageviews, edits, the latest edit timestamp and namespaces (i.e. categories) linked to each article, but also to access the latest news listed in WikiNews to generate exemplary news messages.
\end{itemize}

The extension of text samples by metadata was made in order to examine the following hypotheses:
1. The detection accuracy varies (significantly) across different categories.
2. The higher the edits and/or views on a page are, the lower the detection accuracy will be as the human text will be more sophisticated and better worded.
3. The more recent the last edit timestamp of an article is the lower the detection rate will be as newer information will be less likely to be present in the training data used for GPT-2.

It should be noted that only articles with a minimum length of 1000 characters were taken into consideration in order to filter out many entries that would have diminished the quality of the dataset (e.g. entries that only have a redirect notice [ref]).
