\section{Introduction}
\label{ch:introduction}

\textit{Two plus two is}... four. That is one of the presumable ways a human would continue this sentence. OpenAI's most recent text generator, however, has other ideas in mind:

\textit{Two plus two is...}
\begin{quote}
	...a three, five is a six, seven... \\
	...the result of a simple equation, and the... \\
	...four, and two plus three is four...
\end{quote}
These answers were generated by what researchers and various media sources initially referred to as a language model ``too dangerous'' to be released right after its creation on February of 2019~\footnote{\url{https://openai.com/blog/better-language-models/}, \url{https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction}}. Eventually, its creators published \glsunset{gpt2}\gls{gpt2}, a machine learning system capable of generating human-like text. And even though the aforementioned examples show that machine learning methods are still not able to understand natural human language as a whole, \gls{gpt2} has been able to deceive human readers with its high-quality text generations and has thus attracted increased attention to the field \gls{nlp}, especially the subfield of text generation, since.

% Rise of unstructured data + aptitude of deep learning for solving it
With the rise of mobile and social platforms, the spread of internet connectivity and the ease of access to information, the significance of \textit{Big Data} is more present than ever. Three main attributes form the Big Data phenomenon: Firstly, there is volume, which denotes the huge amount of available data - the analyst firm International Data Corporation (IDC) predicts that the `world's data' will grow to 175 zettabytes in 2025~\footnote{\url{https://www.bernardmarr.com/default.asp?contentID=1846}}. Then, there is velocity, which refers to the speed of data creation and transportation - latency between New Jersey (US) and Slough (England) can be as low as 59 ms~\footnote{\url{https://www.lightwaveonline.com/network-design/high-speed-networks/article/16651312/hibernia-express-transatlantic-submarine-cable-network-ready-for-service}}. And finally, there is variety, which constitutes the many different sources and formats of data - user-generated images and videos, as well as sensor or GPS data from smartphones, are examples for data types that complement the more traditional text and relational data. \\
The greater significance of Big Data fuels the demand from organizations to make use of the evergrowing amount of information in the light of data-driven decision making. Making sense and creating value out of large and heterogeneous data sets requires the ability to recognize patterns and subsequently derive recommendable actions from them. These are precisely the fields of work that fall within the scope of \textit{machine learning}. In recent years, its subset \textit{deep learning} has proven that machines are capable of learning complex structures through the representation of the world as a nested hierarchy of concepts and layers.
% (downloading this information at the average current internet connection speed would take you 1.8 billion years)

% Importance and research progress of NLP and language modeling + mention numbers to show significance in business and research (maybe number of publications)
% Intro about difficulty to govern text generation + Positive and negative applications of nlp (especially w/ use of LMs) + End with importance detection of synthetic text
Over the last years, deep learning has yielded state-of-the-art results in the area of ~\gls{nlp}. Here, machine learning techniques are applied to the analysis and synthesis of natural language and speech. With applications such as machine translation, speech recognition or text generation, the importance of \gls{nlp} has widely been recognized. Deloitte Insights titles it as a \textit{gamechanging technology}~\footcite{briggs2019tech}. McKinsey regards NLP as particularly useful for the processing of unstructured text, found e.g. in medical records, newspaper articles or social media articles~\footcite{chui2017artificial}. Fisher et al.~\footcite{doi:10.1002/isaf.1386} also identified use cases for companies in accounting, auditing or in the finance sector. However, \gls{nlp} is a double-edged sword: With the power to synthetically generate human-like text comes the risk of misuse such as increased dissemination of false news. The motivation for malicious text generation ranges from creating fake websites in order to offset search engine indexes, to generating fake scientific papers up to the creation of bot networks that are able to spread misinformation through comments and posts in social media and news articles. At the latest since it became known that Russia had interfered with the US presidential elections through a systemic spread of false news in the internet, the presence of \textit{`fake news'} is ubiquitous~\footcite{allcott2017social}. While most fake news are still manually generated, AI-generated fake news are inevitable. As newer text generation methods, that are specifically designed to produce human-like text, emerge, conventional methods are not sufficient for synthetic text detection and more sophisticated approaches are required.

% Where does this thesis fit + What is its goal (+ state hypothesis?)
This thesis provides an overview of existing detection tools and mechanisms. Furthermore, it implements extensions for improving these models through domain-specific metadata. With Wikipedia as a source for literature written by humans, the first 60 characters of each \textit{preprocessed} entry were used to build synthetic text paragraphs. The resulting data set of human-machine text pairs with an amount of 412,482 samples (160MB) was used to benchmark linear detection methods, basic feedforward networks and fine-tuned language models with additional decision layers. As for the text generation, different parameters were employed to gain insights into which features yield the most convincing texts. Then, the variation of the detection rate when providing domain-specific knowledge such as the number of page views, categories and the latest edit was analyzed. With the learned models, experiments were conducted to see if the models are suited to expose generated news articles about ongoing topics, as this is a field prone to be targeted by malicious actors.
% Thesis Topic \\
% Detection of synthetically generated text \\
% Problem: Synthetic text generators like OpenAIâ€™s GPT-2 are already capable of creating text that is hardly distinguishable from human-created text. As these generators are open-source they are prone to be misused, e.g., for propaganda purposes. \\
% Goal: Provide an overview of existing detection mechanisms and prototypically implement extensions for improving these models (e.g., additional meta-data). \\
% Data: Generated text from GPT-2 and collected human texts

The structure of this thesis is outlined in the following. Chapter~\ref{ch:background} covers the necessary theoretical concepts of deep learning and language modeling for the comprehension of later chapters. Afterwards, the methodology and approach used for the creation of a suited dataset and the detection of synthetic text is explained in Chapter~\ref{ch:methodology}. Results of the used classification methods and the developed extensions for improved accuracy are presented in Chapter~\ref{ch:results}. The contributions and limitations of this thesis as well as future and necessary research opportunities are elucidated in Chapter~\ref{ch:discussion}.
