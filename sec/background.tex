\section{Background}
\label{ch:background}

This chapter serves to explain the foundations of natural language processing (NLP), especially the subpart of
language modeling, needed to understand the problem and methodology used in this thesis. The two main building blocks
of this thesis are the examination of a state-of-the-art language model (LM) and its ability to generate high quality,
human-like text on the one hand and methods to distinguish such generations from human written text on the other hand.
In order to understand the problem and methodology used in this thesis, this chapter explains the foundations
of NLP (ch.~\ref{sec:history_of_language_modeling}), especially the subpart of language
modeling, and the foundations of deep learning (ch.~\ref{sec:deep_learning_in_machine_learning}) especially under the aspect of sequence
classification.

\subsection{Deep learning in machine learning}
\label{sec:deep_learning_in_machine_learning}

This section aims to firstly embed the field of deep learning, which is the current driver of the latest progress in the field of \textbf{natural language processing (NLP)}\footcite{Deng2018}, into machine learning. The first subsection (ch.~\ref{sub:definition}) will define machine learning and especially deep learning and will go over the most common terminology found in these fields. Afterwards (ch.~\ref{sub:neural_networks}), neural networks as the key concept for deep learning will be introduced. Later on (ch.~\ref{sub:recent_developments}) recent developments will be presented and the chapter will be concluded by giving an overview of practical applications of deep learning (ch.~\ref{sub:practical_applications}). The notation used throughout this chapter mostly follows the one used by Bishop~\footcite{bishop2006pattern} in his book “Pattern Recognition and Machine Learning“. Here scalar values are noted as lowercase letters (e.g. $ x $), vectors as bold lowercase letters (e.g. $ \pmb{b} $) and matrices as bold uppercase letters (e.g. $ \pmb{W} $).

\input{sec/deep-learning/definition.tex}

\input{sec/deep-learning/neural_networks.tex}

\input{sec/deep-learning/recent_developments.tex}

\input{sec/deep-learning/practical_applications.tex}

\subsection{History of language modeling}
\label{sec:history_of_language_modeling}

Natural language processing (NLP) is “an area of research and application that explores how computers can be used to
understand and manipulate natural language text or speech to do useful things”~\footcite{doi:10.1002/aris.1440370103}.
The applications of NLP such as speech recognition, sentiment analysis,
question answering and others are numerous~\footcite{DBLP:journals/corr/GattK17}.
Moreover, these applications are already being heavily used by industry and
consumers alike e.g. in the forms of digital voice assistants, sentiment analysis for recommender systems
and browser search bars~\footcite{8012330,10.1145/3064663.3064672,GoogleSearch}. The subcomponent of NLP
needed when it comes to tasks like machine translation, predictive typing or summarization that involve either
generating text or estimating the probability of text is called language modeling. The following notation mostly
follows the one from the CS224 Stanford Natural Language Processing with Deep Learning lecutre by Chris Manning~\footnote{\url{https://web.stanford.edu/class/cs224n/index.html}}. The following chapters will cover word representation (ch.~\ref{sub:word_representation}) as the foundation for language models, followed by a brief explanation of the basics of language models (ch.~\ref{sub:foundations_of_language_modeling}) and will end with the most common and most used language model types and architectures (ch.~\ref{sub:n_gram_models} - ch.~\ref{sub:neural_language_models}).

\input{sec/history-of-lm/word-representation.tex}

\input{sec/history-of-lm/foundations-of-language-modeling.tex}

\input{sec/history-of-lm/n-gram-models.tex}

\input{sec/history-of-lm/neural-language-models.tex}

\input{sec/history-of-lm/attention-and-transformers.tex}
