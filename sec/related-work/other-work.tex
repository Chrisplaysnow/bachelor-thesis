\subsubsection{Other work}
\label{sub:other_work}

% After releasing their own 1.63 billion-parameter \gls{lm} \gls{ctrl} which provides users with fine-grained control over text generation~\footcite{keskar2019ctrl}, the Salesforce Research team also decided to tackle the tasks of classifying text as `genuine' or `generated'. In their paper ``Limits of Detecting Text Generated by Large-Scale Language Models''~\footcite{varshney2020limits} they present their framework [DETAILS ABOUT FRAMEWORK MISSING].
% Besides that, several other studies have used supervised learning to develop classifiers for this task.
Beyond that, several other studies have used supervised learning to develop classifiers for this task. Solaiman et al.~\footcite{solaiman2019release}, the publishers of \gls{gpt2} as well as different members of the \gls{ctec} and others released a `manifest' in which they discuss the benefits of a staged release and propose guidelines to enhance coordination and responsibility in \gls{ai}. According to this work, there should be enough time between model releases in order to be able to conduct risk and benefit analyses. Furthermore, the authors found out that the detection accuracy of current \gls{lm}s can be very sensitive to the sampling method of the test examples, depending on which sampling method the training examples used.
% Another paper~\footcite{DBLP:journals/corr/abs-1906-03351} proposes the use of an \gls{ebm} framework (i.e. based on generative models). Its authors argue that because \gls{ebm}s are not limited to scoring a single word at a time these models might prove to be better detectors of generated text than auto-regressive models. In addition to that, \gls{ebm}s achieve good generalization when trained on large datasets.
Researchers at the \gls{mit} acknowledge recent studies on automatic fake news detection that either base their detection on the provenance of the article or that analyze the content and verify it against reliable sources, but they find that these approaches do not take account for the scenario where generators are also used for producing legitimate text, nor for more sophisticated attackers that use a generator to create malicious content while at the same time keeping minimal distributional differences from a legitimate source. To highlight this fact, Schuster et. al demonstrated the ineptitude of current detectors by considering auto-completion of news articles with correct information and also by considering an attacker that uses probabilities assigned by a language model to guide minimal edits that modify the correctness of an article's statements. Hereby, defenses that perform detection of auto-generated extremely well can be deceived by generator-based attackers. The authors, therefore, conclude that it is of utmost importance to develop better detectors against \textbf{all} types of fake news by e.g. building up diverse and challenging datasets. They also recommend the creation of a benchmark that represents content's veracity in a wide range of human-machine collaboration applications - taking not only whole article generation into consideration but also hybrid writing and editing. According to them, it is more important to inspect potential fake news for veracity rather than provenance.
The paper ``Human and Automatic Detection of Generated Text''~\footcite{ippolito2019human} thoroughly investigates which choices such as sampling strategy and text excerpt length can impact the performance of not only automatic detection methods but also human raters. Their findings suggest that there is a trade-off between automatic discriminator detection rate and human detection rate. For the automatic discriminator detection they first employed simple discriminators, such as bag-of-words or token likelihood bucketing which performed significantly worse than the use of a fine-tuned deep classifier such as \gls{bert}. After comparing the results of their fine-tuned classifier with the results annotated by humans the researchers conclude that current \gls{lm}s are able to generate text that is either difficult for humans to detect, or difficult for an automatic discriminator to detect, but not necessarily both. They especially see a limitation in the fact that it is difficult for \gls{lm}s to generate text that is both exciting (i.e. unlikely) and semantically sound at the same time.
