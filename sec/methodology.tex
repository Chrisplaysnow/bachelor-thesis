\section{Methodology}
\label{ch:methodology}

Having established the necessary background knowledge in the previous chapter, experiments undertaken in this thesis can  now be introduced. Section~\ref{sec:dataset} will introduce the dataset and its building process. Then, section~\ref{sec:approach} will describe the applied discriminator models.

\subsection{Dataset}
\label{sec:dataset}

In order to create a dataset suited for the classification task a few aspects had to be considered. \\
As deep learning methods with many parameters were going to be used for the classification task, many labeled training samples were needed. The dataset should ideally be comprised of equal amounts of synthetically and human-generated texts in order to not be biased towards a certain class and thus improve the accuracy of training. The first idea was to look for already built datasets that are freely available as these would not only reduce the time and amount of work needed for the creation of the dataset but also provide a benchmark against other models used on them. Because the examined classification task is fairly novel and powerful language models have just started to emerge in recent years, there is a lack of standardized data sets - prior research often focused on the detection of artificially generated academic papers instead of short texts~\footcite{lavoie2010algorithmic}. Furthermore, the incentive of using metadata related to text snippets and inspecting the changes in detection accuracy through it led to the motivation of building a new dataset. For this purpose, the following sources of human-created text were inspected - taking different aspects like data availability, extensibility by metadata, the potential for use of text generation and minimal overlap with the pretrained GPT2 Model into account: Wikipedia, Twitter, reviews (e.g. Amazon, IMDb), Reddit comments and news from the Reuters corpus.

With everything taken into consideration, Wikipedia articles were chosen as the best fit for this work. Reasons for this choice were the ease of access, the vast amount of data entries (currently there are more than 6 million articles in the English Wikipedia\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Size_comparisons\#Wikipedia}}), the extensibility by metadata such as pageviews or categories and most importantly the fact that the ready-to-use GPT-2 model was explicitly not trained on any Wikipedia article since "it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks."~\footcite[3]{radford2019language}. The reviews and Reddit comments datasets were not chosen because the metadata was not seen as decisive in improving detection quality. The disadvantage of the Reuters Corpus was that the training dataset used for the ready-to-use GPT2 model is comprised of many newspaper sources and thus is more likely to generate results that are rather similar to their human written counterparts. Twitter was seen as a promising data source, but the access to its public API was ended which is why it could not be further considered.

\input{sec/dataset/extraction_process.tex}

\input{sec/dataset/generation_parameter_combination.tex}

\subsection{Approach}
\label{sec:approach}

After setting up the dataset, the task of detecting synthetic text remains. Hereby, this thesis puts a particular emphasis on comparing the performance of classification methods throughout all complexity levels. To achieve this, available models were split into three different categories: In \textbf{category 1}, a first baseline using more elemental statistical models such as a \textit{logistic regression} or a \textit{\gls{svm}}. Here, the discriminator methods were we based upon features grounded in practice, that have been used in research throughout many years, such as \textit{TF-IDF} scores. The ideas for the use of these features and for the model architectures were taken from previous papers that dealt with this subject~\footcite{lavergne2008detecting,beresneva2016computer}. Having set up this baseline, \textbf{category 2} includes neural architectures that are specifically designed to process sequence input data. These models (\gls{rnn}, \gls{lstm}, \gls{cnn}, Transformer) employ complex neural layers which should result in the possibility to build more complex input representations and therefore also achieve a higher detection rate. Including the whole text as an input feature requires some preprocessing steps detailed in subsection~\ref{sub:word_representation}, mainly the tokenization of words and then the transformation into word embeddings. \textbf{Category 3} consists of the most complex detectors of currently available \gls{lm}s which were fine-tuned for the detection for the detection of the generated content. Firstly, by only providing text and later on by also providing the additional metadata. Because of their specialization, it is expected that these fine-tuned \gls{lm}s will perform best.

Like with the dataset creation process, most of the `explorative programming' was done in either local jupyter notebooks or Google Colaboratory. Especially the descriptive analysis process which requires no GPU-intensive tasks was done locally. The linear models and the deep feedforward networks were mainly trained on Google Colaboratory and the sophisticated ~\gls{lm}s were fine-tuned using the existing Google Cloud infrastructure mentioned above.

\subsection{Infrastructure}
\label{sec:infrastructure}

The dataset creation and the experiments were conducted using infrastructure suitable to handle complex computations and large datasets. The text generation process of \gls{lm}s as well as the training of deep neural networks typically requires the parallelizable computing power of graphics processors. In addition, large data files need to be stored and processed. To create the dataset given the aforementioned parameter configurations (subsection~\ref{sub:generation_parameter_combination}), the first pipeline runs took place in Google Colaboratory\footnote{\url{https://colab.research.google.com/notebooks/intro.ipynb}}, a free jupyter notebook cloud hosting platform which provides users with a GPU, in this case, an Nvidia Tesla K80\footnote{\url{https://www.nvidia.com/en-gb/data-center/tesla-k80/}}. For the generation of synthetic text, the large GPT-2 model (774M parameters) was used. The generation of each text snippet took about 11 seconds. As soon as the pipeline was fully functioning the jupyter notebooks were converted into python modules and the code was transferred to a Google Cloud\footnote{\url{https://cloud.google.com/}} deep learning virtual machine\footnote{\url{https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning?_ga=2.99839453.-2121832178.1549923708}} (VM) with 13GB of memory, a 100GB standard persistent disk, and 2 Nvidia K80 GPUs. Google Cloud was elected for the reason of providing users with a free credit of 300\$, while other cloud computing providers such as AWS\footnote{\url{https://aws.amazon.com/}} or Azure\footnote{\url{https://azure.microsoft.com/en-us/features/azure-portal/}} only provide a free credit of 100\$. Furthermore, the specialized deep learning VM was chosen as it is configured to support common GPU workloads out of the box and removes the task of setting up a high-performance computing environment by having all the needed libraries (e.g. PyTorch, CUDA) and corresponding drivers preinstalled and with a cost of 295.20\$ per month also being less expensive than choosing the same hardware configuration oneself with a cost of about 500\$ per month. The creation of the data points was parallelized and the total time needed for the dataset creation was approximately 14 days. The data points and log files were stored daily in a Google Storage Bucket\footnote{\url{https://cloud.google.com/storage/docs/json_api/v1/buckets}} and could then be downloaded into a local machine for manual inspection. \\
The infrastructure for the training process was similar. In Google Colaboratory the proper functioning of the models and the data preparation processes were tested and the finalized pipeline that iterated through all possible permutations of configurations (e.g. model type, learning rate, embedding function) to find optimal combinations was deployed on another deep learning VM with mostly the same configurations, but instead of having K80 GPUs, T4 GPUs were used as they have approximately the same compute power but consume less energy and are thus cheaper. All in all, the credits of two Google Accounts were used up totaling an infrastructure cost of about 600\$.

