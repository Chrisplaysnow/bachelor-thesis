\section{Methodology}
\label{ch:methodology}

Having established the necessary background knowledge in the previous chapter, experiments undertaken in this thesis can  now be introduced. Section~\ref{sec:dataset} will introduce the dataset and its building process. Then, section~\ref{sec:approach} will describe the applied discriminator models.

\subsection{Dataset}
\label{sec:dataset}

In order to create a dataset suited for the classification task a few aspects had to be considered. \\
As deep learning methods with many parameters that need to be optimized were going to be used for the classification task, many labeled training samples were needed. The dataset should ideally be comprised of equal amounts of synthetically and human-generated texts in order to not be biased towards a certain class and thus improve the accuracy of training.

The first idea was to look for already built datasets that are freely available as these would not only reduce the time and amount of work needed for the creation of the dataset but also provide a benchmark against other models used on them. Because the examined classification task is fairly novel and powerful language models have just started to emerge in recent years, there is a lack of standardized data sets - prior research often focused on the detection of artificially generated academic papers instead of short texts~\footcite{lavoie2010algorithmic}. Furthermore, the incentive of using metadata related to text snippets and inspecting the changes in detection accuracy through it led to the motivation of building a new dataset.

For this purpose, the following sources of human-created text were inspected - taking different aspects like data availability, extensibility by metadata, the potential for use of text generation and minimal overlap with the pretrained GPT2 Model into account: Wikipedia, Twitter, reviews (e.g. Amazon, IMDb), Reddit comments, Reuters Corpus.

With everything taken into consideration, Wikipedia articles were chosen as the best fit for this work. Reasons for this choice were the ease of access, the vast amount of data entries (currently there are more than 6 million articles in the English Wikipedia\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Size_comparisons\#Wikipedia}}), the extensibility by metadata such as pageviews or categories and most importantly the fact that the ready-to-use GPT-2 model was explicitly not trained on any Wikipedia article since "it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks."~\footcite{radford2019language}. \\
The reviews and Reddit comments datasets were not chosen because the metadata was not seen as decisive in improving detection quality. The disadvantage of the Reuters Corpus was that the training dataset used for the ready-to-use GPT2 model is comprised of many newspaper sources and thus is more likely to generate results that are rather similar to their human written counterparts. Twitter was seen as a promising data source, but the access to its public API was shut down which is why it could not be further considered.

\input{sec/dataset/extraction_process.tex}

\input{sec/dataset/generation_parameter_combination.tex}

\input{sec/dataset/data_building.tex}

\subsection{Approach}
\label{sec:approach}

After setting up the dataset, the task detecting synthetic text remains. Hereby, this thesis particularly puts an emphasis on the performance comparison of fundamentally different approaches to this task. Especially the suitability of deep learning methods and fine-tuned language models in comparison to simpler classification methods is to be analyzed. To achieve this objective, a first baseline was determined using logistic regression models. Here, features that were optimized by the discriminator were especially the factors of metadata values and basic textual properties such as the types (and according counts) of tokens. The ideas for the use of these features and for the model architectures were taken from previous papers that dealt with this subject~\footcite{lavergne2008detecting,beresneva2016computer}. Having set up this baseline, the same input information was provided to more complex models, namely variations of deep feedforward models. These models employ standard feedforward layers which should result in the possibility to build more complex input representations and therefore also achieve a higher detection rate. The most complex detectors were some of the currently available language models which were fine-tuned for the detection of the generated content. Firstly, by only providing text and later on by also providing the additional metadata. Including the whole text as an input feature requires some preprocessing steps detailed in subsection~\ref{sub:word_representation}, mainly the tokenization of words and then the transformation into word embeddings. Because of their specialization It is expected that these fine-tuned language models will perform best.

Like with the dataset creation process, most of the `explorative programming' was done in either local jupyter notebooks or Google Colaboratory. Especially the descriptive analysis process which requires no GPU-intensive tasks was done locally. The linear models and the deep feedforward networks were mainly trained on Google Colaboratory and the sophisticated ~\gls{lm}s were fine-tuned on the existing Google Cloud infrastructure mentioned above.




Given the aforementioned parameter configurations, the first pipeline runs took place in Google Colaboratory\footnote{\url{https://colab.research.google.com/notebooks/intro.ipynb}}, a free jupyter notebook cloud hosting platform which provides users with a GPU, in this case, an Nvidia Tesla K80\footnote{\url{https://www.nvidia.com/en-gb/data-center/tesla-k80/}}. For the generation of synthetic text, the large GPT-2 model (774M parameters) was used. The generation of each text snippet took about 11 seconds. As soon as the pipeline was fully functioning the jupyter notebooks were converted into python modules and the code was transferred to a Google Cloud\footnote{\url{https://cloud.google.com/}} deep learning virtual machine\footnote{\url{https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning?_ga=2.99839453.-2121832178.1549923708}} (VM) with 13GB of memory, a 100GB standard persistent disk, and 2 Nvidia K80 GPUs. Google Cloud was elected for the reason of providing users with a free credit of 300\$, while other cloud computing providers such as AWS\footnote{\url{https://aws.amazon.com/}} or Azure\footnote{\url{https://azure.microsoft.com/en-us/features/azure-portal/}} only provide a free credit of 100\$. Furthermore, the specialized deep learning VM was chosen as it is configured to support common GPU workloads out of the box and removes the task of setting up a high-performance computing environment by having all the needed libraries (e.g. PyTorch, CUDA) and corresponding drivers preinstalled and with a cost of 295.20\$ per month also being less expensive than choosing the same hardware configuration oneself with a cost of about 500\$ per month.

